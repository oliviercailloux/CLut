\RequirePackage[l2tabu, orthodox]{nag}
\documentclass[version=last, pagesize, twoside=off, bibliography=totoc, DIV=calc, fontsize=14pt, a4paper, french, english]{scrartcl}
%need to tell arXiv to process with pdflatex with the following line. Then, "Package hyperref Message: Driver (autodetected): hpdftex." Otherwise, urls don’t break, which messes the bibliography. See how to solve this (using latex?)
\pdfoutput=1
\input{preamble/packages}
\input{preamble/math_basics}
\input{preamble/math_mine}
\input{preamble/redac}
\input{preamble/draw}
\input{preamble/acronyms}

%\setbeamertemplate{headline}[singleline]

\begin{document}
\title{%
	Learning to recommend justifiably%
}
\author{Florian Yger}
\author{Olivier Cailloux}
\affil{Université Paris-Dauphine, PSL Research University, CNRS, LAMSADE, 75016 PARIS, FRANCE\\
	\href{mailto:olivier.cailloux@dauphine.fr}{olivier.cailloux@dauphine.fr}
}
\makeatletter
	\hypersetup{
		pdfsubject={collaborative learning},
		pdfkeywords={machine learning},
		pdfauthor={Olivier Cailloux, Florian Yger}
	}
\makeatother
\maketitle

\section{Goal: justifiable recommendations}
Recommender systems aim at recommending some item as most appropriate for a given user.
In this article, we introduce the concept of justifiable recommendations. Those are recommendations that are accompanied with explicit principles whose acceptance is sufficient to justify the recommendation. We make this definition precise by relating it to the “deliberated” preference of the user; and describe an hypothetic situation where such recommendations could be obtained using classical machine learning techniques.

We start from a setting that is classical in the field of collaborative filtering. A set of items is defined among which a user wants to choose. The system aims at recommending an item to the user on the basis of which items the user or “similar” users have appreciated in the past.

It is nowadays considered important, in many situations, to accompany such recommendations with explanation, justification, or argumentation. It has been remarked that these concepts are not equivalent, and that different situations require different kinds of justification accompanying the recommendation. A very simple example of a justification can be considered as implicitly accompanying any recommendation of a classical collaborative filtering system: that it “predicts well on average”. When made explicit, the justification could develop as something like the following. Given many users and many recommendations, the recommender system (assumed here as having been validated using classical techniques) has been shown to “closely” predict the user’s appreciations of the items, on average. And the system recommends what it predicts will be well appreciated by the current user. Therefore, the given recommendation is “probably” an appropriate one. In some cases, such a justification may not be solid enough, for example, if the stakes are high or justice considerations enter into the picture. More importantly, prediction of appreciation is not necessarily the best grounds for justifying a recommendation, as we will argue here. The literature in machine learning has of course proposed numerous justifications or explanations techniques more subtle than the naïve example taken in this introductory paragraph, which will be reviewed shortly and contrasted to our proposal (to be done…).

In this article we are interested in producing recommendations that are accompanied with a principled justification, grounded on decision theory. We mean by this that it takes the form of a set of principles that, if accepted, lead logically to the recommendation. Such a justification makes it clear what it is enough for the user to accept in order to be convinced that the recommendation is appropriate for her. We believe such an approach to justification is original in the field of collaborative learning (to be verified).

Our proposal applies to a specific case of recommendation, described as follows. In our case of interest, the items among which the user aims to choose can be exhaustively described by a set of objective descriptors of the items, known a priori, and which each evaluate the desirability of the items on some aspect. Such descriptors are here called criteria. Exhaustively described means that nothing else than the performance of the items on the criteria is relevant for the recommendation. In this case, the recommendation task is said to be justifiable, given that set of criteria. Thus, a recommendation task is justifiable, given a set of criteria, when two items are equally recommendable whenever they are equally valued on this set of criteria.

Consider as an example the choice of an apartment to rent for holidays. The set of criteria might be: surface of the living room, number of beds, distance to city center, modernity of equipment, presence of a washing machine, price. The recommendation task is justifiable given these criteria if, when two apartments are equally valued on all these criteria (have the same surface of the living room, the same number of beds, and so on), they are equally recommendable. Thus, when the set of criteria completely describes the desirability of an apartment.

We acknowledge that justifiability is, in many case, an approximation. We merely assume in this article that it is an appropriate approximation to consider some recommendation tasks as justifiable given some known set of criteria. Other examples include: buying a smartphone, buying a computer, choosing a flight to reach a given place, or even, possibly, choosing who to give a “best student” award among a given classroom, hiring a new employee, or choosing a university to study at.

By contrast, a recommendation task is not justifiable (or, more precisely, it is unknown how to make it justifiable) if no set of criteria is known that fully describe the desirability of an item. For example, a choice of movie to watch this evening. Note that a movie is certainly describable on some set of descriptors (for example, the singleton set including its serial number, or a set including its title, release date, and other attributes that together uniquely identify a movie), but those are not criteria as defined above, as those descriptors do not each evaluate the desirability of a movie on some aspect.

When a recommendation task is justifiable, particularly interesting links between machine learning and decision theory can be developed. 
One interest of providing recommendations based on decision theory is that it then rests on principles usually considered sound, and in any case fully explicited.

\subsection{Prediction of deliberated preferences}
The class of functions that we describe in this article, coming straight from decision theory, do most likely not constitute the best class of functions among which to search for good predictors of what people like, in many decision problems, including those of interest to us as described above. We acknowledge, and even, put forward, that our recommender system might not optimize predicted appreciation as well as a classical collaborative learning recommender system.
In this section we will argue that depending on the situation, this might be a strength rather than a weakness.

We consider that for some decision problems, the best recommendation is not the one that corresponds to what an accurate predictor would consider as optimizing the appreciation of the user. This is because the prediction generally targets the usual, non reflexive behavior of average users, whereas a recommendation possibly should take into account thoughful preferences. The usual behavior of a set of recruters in an enterprise may, for example, exhibit (possibly unconscious) bias against some categories of applicants. Similarly for teachers grading students. Or, the items among which to decide are not easily compared by “level of appreciation” (for example, which cause should I donate money to?).

In fact, this simple remark is not new, and has been put forward in various recent articles (todo find refs), although the fact that it fundamentally changes the goal of the machine learning procedure has never (to our best knowledge, and to be verified) really been analyzed.

More generally, experimental psychology suggests that users do not always behave naturally as they would behave when thinking more about the problem.

Thus, we claim that for some problems, the best recommendation is one that is based on sound decision principles, even (and especially) when this does not match the item that the user would have naturally opted for. This may be because the user would not have thought hard enough about the problem, or because the user lacks knowledge. Basing a recommendation on decision theory might also raise items to the knowledge of the user that she simply would have never considered, therefore increasing serendipity (more on this below).

These remarks raise two important issues. First, there is a danger of being paternalist, manipulative, or even authoritarian, by considering that we (scientists) know better than the user what is good for him. Second, how are we (as scientists) to validate and compare recommendation systems if not simply by reducing the recommendation task to a prediction task?

The answer to both issues come from a recent proposal of \citet{cailloux_formal_2017}, which we follow here and instantiate in the setting of collaborative learning. Instead of considering the principles of decision theory, on which we propose to ground the recommendation, as principles that must necessarily be accepted, under pain of being considered irrational (which raises the question: who decides which principles are applicable when?), we propose to consider such principles as falsifiable descriptions. Thus, they can be tested, which also contributes to address the second issue above. The thing that such principles describe and ought to predict is not, contrary to custom in machine learning, the usual behavior of the user, but her thoughtful judgment. This has been termed her deliberated judgment in the above mentioned article.

In this introductory article, we cannot go all the way to describing in detail how recommender systems based on decision principles considered as falsifiable would be tested (and possibly falsified). (Much details are given in the companion article cited above, though they are described generally and not in the specific setting of machine learning.) The big picture, however, is the following. Once a recommender system is proposed based on some decision principle, it should be able not only to recommend items to users, but also to justify its recommendations, and defend it against other candidate recommendations. This would be done in terms of the underlying preference model. Other candidate recommendations would come from concurrent recommender systems, against which the current system is compared, or it can come from the mind of the user: the user might be surprised by a recommendation and ask the system why the system didn’t rather recommend such other item. The user herself would be the judge of such a test: seeing the justification given by the recommender system, and possibly, the one given by the concurrent recommender system, the user would have to declare which one is convincing (if any). In this way, the user is the person who decides which decision principles are sound in the given situation, rather than anyone else.

To illustrate, we can use transitivity as a famous and simple example of a decision principle. It says “if you prefer item $a$ to item $b$ and item $b$ to item $c$, you ought to prefer item $a$ to item $c$”. A recommender system which has somehow determined (or thinks it has determined) that the user prefers $a$ to $b$ and $b$ to $c$ could use this principle to argue in favor of choosing $a$ rather than $c$. This could be useful in order to defend its recommendation (assuming it is $a$) against a concurrent system that would recommend $c$.

Approaches that produce justifications on the basis of decision-theoretic preference models have started to appear in the literature. Extending such approaches so that they are able to argue and counter-argue against different systems producing contradicting recommendations is a task that is still to be developed, and that we do not undertake here. Our goal in this article, apart from presenting a general agenda in favor of learning decision theoretic recommendation models and validating them differently than usual (as done in this first section), is to suggest a basic approach, applicable in a setting similar to the classical collaborative learning setting, that is able to learn decision-theoretic preference models. Such models could then be fed to techniques of generating justifications, in order to validate the models.

We think a good starting point, in order to direct the learning process of decision models, is to suppose that the usual behavior of the user is a noisy version of its deliberated behavior. Thus, we will attempt to learn the decision model that fits as closely as possible its usual behavior. More elaborated developments of our proposal would incorporate biases known from experimental psychology to systematically attempt to correct possible mistakes of the user (the word “mistake” referring, again, not to an external obejctive norm that would tell how the user ought to behave, but to the difference between her usual behavior and her own deliberated judgment).

Other approaches exist which propose to estimate a decision-theoretic model that best approximates a user’s observed behavior, thus, our proposal is not new in this respect. What is new is our proposal of a research agenda to develop such models not because they would best predict the user’s normal behavior, but because they could best describe her deliberated judgment; and our instanciation to collaborative learning.

\section{Technical approach}
Assume we have a set of users represented by $I$ of size $m \in \N$, a set of items represented by $J$ of size $n \in \N$, a set of observed pairs $O \subseteq I × J$ and a relation $r$ mapping those observed pairs $(i, j) \in O$ to some rating $r_{ij}$, an integer in $\intvl{1, 5}$ (throughout the article, this notation designates intervals in the integers). 
We also have descriptions of the users in a vector space $A$ of dimension $d_A \in \N$: user $i$ is described by $a_i \in A$. 
The items are described by a set of descriptors whose indices are in $\mathcal{C} = [[1, …, c, …, d]]$. The descriptor $B_c: J → X_c, B_c \in \mathcal{B}$ evaluates each item according to a scale $X_c$. 
%We also write $B_{cj}$ for $B_c(j)$, viewing the description information as encoded in a matrix $B$. 
Let $X = \prod_{c \in \mathcal{B}}$ denote the cardinal product space where descriptions of items lie, and let $b_j \in X$ denote the description of an item $j$.

\subsection{Classical collaborative learning}
In classical collaborative learning, we wish to obtain compressed descriptions of the users and the items, $\{u_i, i \in I\}$ and $\{v_j, j \in J\}$, with all $u_i$ and $v_j$ being vectors of some size $k \in \N$. We write $U$ the matrix of size $(m, k)$ that has all $u_i^T$ as rows, and $V$ the matrix of size $(k, n)$ that has all $v_j$ as columns. The compressed descriptions should be such that $r_{ij} \approx u_i^T v_j$.

\subsection{Using given knowledge}
However, we also wish that our compressed descriptions $u_i$ be in a simple relationship with the provided descriptions $a_i$, thus, we search for $u_i \approx f(a_i)$, and similarly, for $v_j \approx g(b_j)$, with $f$ and $g$ “simple”. More precisely, we search for a matrix $W_A$ of size $(k, d_A)$ such that $W_A a_i \approx u_i$, and $W_B$ of size $(k, d_B)$ such that $W_B b_j \approx v_j$. We call $W_A a_i$ and $W_B b_j$ our explainable representation of the users and the items.

We could define $u_i = W_A a_i$ and $v_j = W_B b_j$ and obtain $r_{ij} \approx a_i^T (W_A^T W_B) b_j$, thus we could optimize by searching for the best $(W_A^T W_B)$. But as this could be too constrained, we will also try to relax these constraints and search for a representation that takes into account that $u_i$ should be close to $W_A a_i$, but not necessarily equal.

\subsection{Using criteria}
Let us define $\succeq$, a binary relation on the possible items $\mathcal{J}$, as representing the deliberated preference of the user under hand, to which we recommend. (It is possible that $\mathcal{J}$ be bigger than the currently observed items $J$.) Let us see what conditions are necessary and sufficient for a function in our priviledged class of function to exist that represent her deliberated preferences. Define $\sim$ as the symmetric part of $\succeq$ and $\succ$ its asymmetric part. We assume for simplicity that there are at least three descriptors and that the scales $X_j$ are not “too big”: there exists an injection between each $X_j$ and the real numbers $\R$. These assumptions seem innocuous for the kind of applications we focus on in this article.

Condition 1: $\succeq$ is a complete preorder (also called a weak order): a complete, reflexive, transitive relation.
Thus, $\sim$ is an equivalence relation.

Condition 2: if $b_j = b_{j'}$, then $j \sim j'$.

With $\mathcal{C} = C ∪ \{c\}$ for some $c \in \mathcal{C}$, thus $C$ represents all descriptors but one, let $(x_C, x_c) \in X$ represent the description of an item $j \in \mathcal{J}$ where $x_C \in \prod_{c \in C} X_c$ describes the item $j$ on the criteria $C$ and $x_c$ is the remaining description.

Condition 3: it is possible to define partial preferences $\succeq_c$ for each criterion, complete preorders, such that, all other things being equal, a better value on some criterion $c$, as judged by $\succ_c$, implies a preference for the new item. More precisely, using $\succ_c$ to denote the asymmetric part of $\succeq_c$, for any $C$ representing all criteria but one, and any two items having descriptions $b_j = (x_C, x_c)$ and $b_{j'} = (x'_C, x'_c)$ such that $x'_C = x_C$ and $x'_c \succ_c x_c$, it holds that $b_{j'} \succ b_j$.

Condition 4: Trade-offs between two criteria must be independant of levels on other criteria.

Condition 5: Some condition on the limit behavior of limit tradeoff situations (to ensure derivability).

Theorem (to be checked): if the preference of the user $\succeq$ satisfies all the above conditions, then there exists a value function $v: \mathcal{J} → \R$ and partial value functions $v_c: X_c → \R$ for each criterion $c$ such that for any two items, $j \succeq j' ⇔ v(j) ≥ v(j')$ and $v(j) = \sum_{c \in \mathcal{C}} v_c(b_c(j))$, and the partial value functions $v_c$ are all increasing in $\succ_c$: if $x_c \succ_c x_{c'}$ then $v_c(x_c) \succ_c v_c(x_{c'})$.

\subsection{Predict ratings with logits}
Assume we are given a loss function $L$ that compares the ratings $r_{ij}$ to our approximations of the ratings, for $(i, j) \in O$.

Define $\sigma(\alpha, \beta, x) = \frac{1}{1+\alpha e^{-\beta x}}$.

We want to find $\{w_i^c, i \in I, c \in \mathcal{C}\}, \{\alpha^c, c \in \mathcal{C}\}, \{\beta^c, c \in \mathcal{C}\}$ that optimize the following objective:
\begin{equation}
\min_{w, \alpha, \beta} \sum_{(i, j) \in O} L(r_{ij}, \sum_{c \in \mathcal{C}} w_i^c \sigma(\alpha^c, \beta^c, b_j^c)) + \lambda \sum_{i, c} \norm{w_i^c}_2^2.
\end{equation}
We force $w$ positive. And $\alpha$ is forced positive to respect the preference direction: we assume the data is encoded so that higher $b_j^c$ means preferred performance. Where $b_j^c$ is the performance of item $b_j$ on criterion $c$.

\subsection{Predict ratings}
We could want to check whether we are able to predict the right ratings. In that case, we would assume we are given loss functions $L$, that compare the ratings $r_{ij}$ to our approximations of the ratings, for $(i, j) \in O$; $L_I$, that compare the vectors $u_i$ to our explainable representation of them; and $L_J$, that compare the vectors $v_j$ to our explainable representation of them.

We would want to find matrices $U, V, W_A, W_B$ that optimize the following objective:
\begin{equation}
\min_{U, V, W_A, W_B} \sum_{(i, j) \in O} L(r_{ij}, u_i^T v_j) + \sum_{i \in I} L_I(u_i, W_A a_i) + \sum_{j \in J} L_J(v_j, W_B b_j).
\end{equation}

\subsection{Predict orders}
We are interested in comparing our predictions to orderings (because that’s what we have underhand).

We assume we are given loss function $L$, that compares preorders (transitive and reflexive binary relations), and $L_I$ and $L_J$, as above. We assume we have a preorder $≥_i$ for each user indicating her true preference over some pairs of items among $J$.

We want to find matrices $U, V, W_A, W_B$ that optimize the following objective:
\begin{equation}
\min_{U, V, W_A, W_B} \sum_{i \in i} L(≥_i, \set{(v_j, v_{j'}) \suchthat u_i^T v_j ≥ u_i^T v_{j'}}) + \sum_{i \in I} L_I(u_i, W_A a_i) + \sum_{j \in J} L_J(v_j, W_B b_j).
\end{equation}

\section{Double-strategy}
In the double-strategy, we use different dimensions: $k = d_{A'} + d_U = d_{B'} + d_V$.
The matrix $W_A$ has size $(d_{A'}, d_A)$ and $U$ has size $(m, d_U)$. $W_B$ has size $(d_{B'}, d_B)$ and $V$ has size $(d_V, n)$. $r_{ij} = \mu_i^T \nu_j$ where $\mu_i$ is the $k$-dimensional vector composed of $W_A a_i$ then $u_i$ and $\nu_j$ is composed of $W_B b_j$ then $v_j$.

We should compare this approach to the more classical one.

\section{Using raw item descriptions}
The matrix $W_B$ could be constrained to having mostly zeroes, to make sure that indeed the attributes of the items ($b_j$) are used in their $\mu$-vectorial description. We could even take $d_{B'} = d_B$ and take $W_B$ equal to identity. (Is there any benefit not to do so?)

This suggests the following simple approach: concatenate $b_j$ to $v_j$ to obtain $\nu_j$. Don’t use $W_B$. Obtain $\mu_i$ of size $k = d_B + d_V$, using $W_A a_i$ as its first components.

\section{Factorization machines}
We should compare (theoretically or experimentally) our approach to factorization machines.

\section{Data set: Sushi}
We experiment using the Sushi data set (ref…)

Here $J$ is a set of $100$ sushi types \footnote{$X^*=X_B$ in the original notation.}. The original authors collected the sushi from the menu of 25 restaurants, computed the frequency of each sushi type occurring on a menu, and took the $100$ most frequent. Write $P_J$ the frequency (thus a multiple of $1/25$).

Write $J_\text{top} \subset J$ the $10$ most frequent sushi types \footnote{$X_A$}.

The set $I$ has cardinal $m = 5000$. The authors drew randomly a set of 10 sushi types among $J$, for each user $i$, according to the distribution $P_J$. Let us denote this by $J_i \subset J$ \footnote{$X^B_i$}.

The authors obtained, for each user $i$ in $I$, three preferential informations, defined as strict partial orders (transitive, asymmetric binary relations). First, they collected a preference ordering over the sushi types from $J_\text{top}$ from most to least preferred, with no tie allowed. Let $>^\text{top}_i$ denote that preference ordering \footnote{$O^A_i$} (“top” designates the fact that the ordering is over the 10 most frequent sushi types). Second, they collected a rating, in $\intvl{1, 5}$, for each sushi type in $J_i$. Define $O$ as the set of pairs $(i, j)$ such that $j \in J_i$, and let $r_{ij}$ denote that rating. Third, after two supplementary questions asking how oily the user think each sushi type is and how frequently they eat them, they collected a preference ordering over $J_i$, that we write $>^\text{indiv}_i$ \footnote{$O^B_i$} (“indiv” designates the fact that this relation orders a set of sushi that has been drawn specifically for that user).

Users are described in a space $A$ comprising the attributes $\{$gender, age category, origin-prefecture, origin-region, origin-east/west (binary), current-region, current-east/west (binary), origin-prefecture$\}$. Items are described in a space $B$ comprising the attributes $\{$group, heaviness/oiliness in taste (in $[0, 4]$), frequency of eating (in $[0, 3]$), normalized price (in $[0, 1]$), $P_J(j)$$\}$.

We summarize the preference of $i$ as follows. The ratings over $J_i$ are transformed to preorders $≥^\text{rating}_i$ that tolerate ex-æquos, in the obvious way (better ratings indicate preference). We then merge, for each user $i$, all preferential information, considering contradictions as indicating indifference, and obtain a preorder $≥_i = [≥^\text{rating}_i ∪ >^\text{top}_i ∪ >^\text{indiv}_i]$, where the brackets designate taking the transitive closure and adding the identity relation (to obtain reflexivity). For example, if $>^\text{top}_i$ represents the ordering $(j_1 > j_2 > j_3)$ and $>^\text{indiv}_i$ represents $(j_6 > j_3 > j_4 > j_2)$ and $≥^\text{rating}_i = \emptyset$, then $≥_i$ considers $j_1$ and $j_6$ as strictly preferred to $j_2$ to $j_4$, $j_2$ to $j_4$ ex-æquos, and $j_1$ incomparable to $j_6$.

\section{References}
\bibliography{clut}

\end{document}

