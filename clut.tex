\RequirePackage[l2tabu, orthodox]{nag}
\documentclass[french, english]{da2pl2018}
\pdfoutput=1
\input{preamble/packages}
\input{preamble/math_basics}
\input{preamble/math_mine}
\input{preamble/redac}
\input{preamble/draw}
\input{preamble/acronyms}

%\setbeamertemplate{headline}[singleline]

\begin{document}
\title{%
	Learning argumentative recommenders%
}
\author{Olivier Cailloux}
\affil{Université Paris-Dauphine, PSL Research University, CNRS, LAMSADE, 75016 PARIS, FRANCE\\
	\href{mailto:olivier.cailloux@dauphine.fr}{olivier.cailloux@dauphine.fr}
}
\makeatletter
	\hypersetup{
		pdfsubject={machine learning},
		pdfkeywords={collaborative learning, recommender systems, argumentation, preference, decision theory, xai},
		pdfauthor={Olivier Cailloux}
	}
\makeatother
\maketitle

\section{Introduction}
In a situation where a user has to choose an item among a set of possible items, recommender systems aim at recommending some item as most appropriate for the user.
In the collaborative learning community, this is usually taken to mean: recommend an item among the most preferred for the user. In this article, I want to propose another goal for recommender systems. I keep the global aim of recommending some item as most appropriate for a given user, but I propose a different understanding of appropriateness. The goal I propose is to recommend an item among those that are in the \ac{DP} of the user, and to prove that it does. The \ac{DP} of the user captures what she prefers when considering all arguments in favor or against all possible items. This differs from her intuitive preference in that the latter does not consider arguments. Thus, the goal I propose is appropriate when one is interested to help the user form a deliberated judgment about which item is best, rather than predict which item the user intuitively prefers. Because this will be done using systems that argue, I call the kind of system I define here an \ac{AR}.
Further motivations for this new goal are described in \cref{sec:motiv}, and it is precisely defined in \cref{sec:def}.

Together with this new goal, it is necessary to propose a corresponding measure of the performance of the recommender system (done in \cref{sec:pred}). In the classical approach, measuring the performance of the system amounts to compare its claims (the items it proposes as preferred) to reality (the preference of the user). Because the \ac{DP} of the user is not directly observable, this direct check of correspondance becomes non applicable. However, as I will show, the problem of evaluating the quality of an \ac{AR} can be framed as a prediction problem (although involving different objects), thereby recovering the possibility of empirical validation by confrontation between claims of the system and reality.

\Cref{sec:princ} then sketches one possible way of undertaking this task, applicable in specific contexts: recommend on the basis of explicit principles coming from decision theory. \Cref{sec:litt} provide further references and discussion.

\section{Motivation}
\label{sec:motiv}
This article discusses a notion of preference that possibly changes when confronted to arguments. That is, I assume that the user’s knowledge of which item is best for him is not necessarily fixed a priori, and in particular, that the user may change his mind when presented with arguments in favor of, or counter-arguments against, various items. 

By contrast, the notion of preference considered by classical recommender systems (usually left implicit) is that the user intuitively knows what he prefers, and that this is the right basis for him to decide. This notion can be related to the conception of preference put forth by \citet[p. 16]{von_neumann_theory_1944} in their seminal work. They write that “It is clear that every measurement – or rather every claim of measurability – must ultimately be based on some immediate sensation (…). In the case of utility the immediate sensation of preference (…) provides this basis.” When applied to collaborative filtering, this view suggests that the user does not need help for comparing pairs of item: the job of the system is merely to relieve the user from having to search through the whole set. 
The user, when presented with a pair of items, knows which one he prefers by introspection, possibly after having tried them both. %DEL?
Similarly, when the user has already provided some comparisons of preference, this is the final word about those pairs.

This notion of preference as “intuitive” preference is not always the best basis to ground recommendation. Indeed, the user might be unwilling to judge what is best for her on the sole basis of her unaided intuition. First, there are cases where the user can’t easily try out the items. An example is a non repeatable choice, such as choosing a university to study in. 
Second, the best choice might be the result of a complex thought process that should better ensure, as much as possible, that all relevant arguments have been considered, rather than purely rely on intuition.
Consider the decision of which smartphone, or which computer, or which house, to buy. 
This is also easily the case when a notion of fairness is involved, for example when determining who will receive a prize, or how to best distribute revenue in a society, or to which cause I should donate money.
Third, the user might appreciate being pointed to some non-salient feature of the items under comparison; for example, a user might feel an intuitive attraction to the plane alternative when chosing a transportation means for holidays, seeing that the flight time is only one hour compared to three hours by train, but change her mind when being reminded that the total travel time should be taken into account. Finally, an argument coming from psychology is that unaided intuition is known to be sensible, in some contexts, to framing effects that would perhaps not be considered relevant (by the user himself) when considering arguments and counter-arguments \citep{kahneman_thinking_2013}. An example that involves several kinds of the just enumerated concerns is obtaining a decision procedure for credit requests in a bank: the user probably wants to involve fairness considerations in order to avoid (possibly unconscious) appearance of unjust discriminations; and even if only profit considerations are to bear, the user might want the procedure to go beyond reflecting the bare intuition of an expert.

\section{Definition of deliberated preference}
\label{sec:def}
This section describes the new understanding of preference I propose, called the \ac{DP} of the user (it is a simplification of the recently proposed concept of deliberated judgment \citep{cailloux_formal_2018}).

I consider given a set of items $\allitems$ among which a user $\usr$ wants to choose. I also consider given a set $\allargs$ containing arguments that may possibly help $\usr$ form a deliberated opinon about which item is best for her. Elements of $\allargs$ are not detailed and can be conceived as strings in a natural language. An example of an argument is $\ar = $ “Item $\itm$ is better than item $\itm'$ because $\itm$ has a good performance on criteria ‘price’ and ‘speed’ while item $\itm'$ has a good performance only on criterion ‘aspect’, which you do not consider important”.

The \ac{DP} of $\usr$ is defined as the subset of items that are considered by $\usr$ as having no item strictly preferred to them, considering all arguments in $\allargs$. To define this properly, I assume we can observe the reaction of $\usr$ to arguments, as follows. 

Define $\ibeats$ as a binary relation over $\allitems × \allargs$, representing the results of the following experiments. Present $(\itm, \ar)$ and $(\itm', \ar')$ to $\usr$ and let her decide, considering the arguments $\ar$ and $\ar'$, which item among $\itm$ and $\itm'$ she prefers, if any. 
Define $(\itm, \ar) \ibeats (\itm', \ar')$ iff $\usr$ strictly prefers $\itm$ to $\itm'$, given $\ar$ and $\ar'$. Note that if no strict preference holds, $\ibeats$ does not relate these two pairs (thus $(\itm, \ar) \nibeats (\itm', \ar')$ and $(\itm', \ar') \nibeats (\itm, \ar)$). 
The relation $\ibeats$ plays here the role of the basic observable \citeauthor{von_neumann_theory_1944} talk about (see \cref{sec:motiv}).

The \ac{DP} $\uitems \subseteq \allitems$ of $\usr$ in that situation contains an item $\itm$ iff $\forall (\itm', \ar') \in \allitems × \allargs, \exists \ar \in \allargs \suchthat (\itm', \ar') \nibeats (\itm, \ar)$.

\section{Prediction of deliberated preference}
\label{sec:pred}
An \ac{AR} should be able, given a user, to single out some items as being in his \ac{DP}, and some items as being not in his \ac{DP}, and to prove its claims by arguing correspondingly. 
Formally, an \ac{AR} $\eta$ has a \emph{scope} $\allusers$, representing a set of users it claims to be able to predict the \ac{DP} of, and gives, for a user $\usr \in \allusers$, a tuple $(\mitems, \mdef, R_\eta, \matt)$, where $\mitems \subseteq \allitems$ is a set of items that $\eta$ claims are among the \ac{DP} of $\usr$, $\mdef: \mitems × \allitems → \allargs$ is an argumentation strategy used to defend items in $\mitems$, $R_\eta \subseteq \allitems × \allitems$ is a binary relation that contains pairs of items $(\itm, \itm')$ such that $\eta$ claims that $\usr$ deliberately prefers $\itm$ to $\itm'$, and $\matt: R_\eta → \allargs$ is an argumentation strategy used to support the claims represented by $R_\eta$. (Elements of this tuple depend on $i$, though this is omitted from the notation.)

An \ac{AR} is valid if its functions $\mdef$ and $\matt$ indeed justify its claims. Informally, it is required of $\mdef$ that, when given an item $\itm'$ challenging an item $\itm \in \mitems$ considered by $\eta$ as being in the \ac{DP} of $i$, $\mdef$ produces an argument that successfully defends $\itm$, whatever the argument given in favor of $\itm'$. More precisely and completely, an \ac{AR} $\eta$ is said to be \emph{correct} about a user $\usr$ iff $\forall \itm \in \mitems, \itm' \in \allitems, \ar' \in \allargs: (\itm', \ar') \nibeats (\itm, \mdef(\itm, \itm'))$ and $\forall (\itm, \itm') \in R_\eta, \ar' \in \allargs: (\itm, \matt(\itm, \itm')) \ibeats (\itm', \ar')$. An \ac{AR} is \emph{valid} iff it is correct about all users in its scope.

Importantly, validity implies the correctness of the claims concerning the \ac{DP} of $\usr$. This fact is stated formally below, with $\mnitems = R_\eta(\allitems) = \set{\itm' \in \allitems \suchthat (\itm, \itm') \in R_\eta \text{ for some } \itm \in \allitems}$. (The proof is omitted.)
\begin{fact}
	If an \ac{AR} $\eta$ is valid, then $\forall \usr \in \allusers: \mitems \subseteq \uitems \text{ and } \mnitems \subseteq \allitems \setminus \uitems$.
\end{fact}

This notion of validity can be used to \emph{compare} \acp{AR}, in the following way. Given two \acp{AR} $\eta$ and $\eta'$, consider a user $\usr$ in both their scopes and obtain two tuples, $(\mitems, \mdef, R_\eta, \matt)$ and $(\mitems[\eta'], \mdef[\eta'], R_{\eta'}, \matt[\eta'])$. Assume they strongly disagree on the prediction of the \ac{DP} of $\usr$, meaning that for some item $\itm \in \allitems$, the first system claims that $\itm$ is in the \ac{DP} of $\usr$, thus $\itm \in \mitems$, and the second system claims it is not, thus $\exists \itm' \suchthat (\itm', \itm) \in R_{\eta'}$. Suffices now to let the two systems play against each other and use $\usr$ as a judge. That is, we obtain an argument from the first system in defense of its claim, $\ar = \mdef(\itm, \itm')$, and an argument from the second system, $\ar' = \matt[\eta'](\itm', \itm)$. We present $(\itm, \ar)$ and $(\itm', \ar')$ to $\usr$ and accordingly obtain $(\itm', \ar') \ibeats (\itm, \ar)$ or $(\itm', \ar') \nibeats (\itm, \ar)$. In the first case, the first system is invalidated, in the second one, the second system is invalidated.

\section{Recommend on the basis of explicit principles}
\label{sec:princ}
Consider a collaborative learning setting, where we have collected ratings from users on items, assumed to represent their intuitive rather than deliberated preference. 
A simple approach to build a proof-of-concept \ac{AR} in such a context is to represent the \ac{DP} of $\usr$ as a weak order ${\succeq} \subseteq \allitems × \allitems$ corresponding to $R_\eta$ and whose maximal elements correspond to $\mitems$ (though the definition tolerates other possibilities).
Assume that the \ac{DP} of the user can be represented by a decision-theoretic model: because such models are conceived for grounding decisions in sound principles, they might be adequate to model deliberated preferences. % (as opposed to intuitive ones). %DEL?
For concreteness, assume it is \ac{MAVT} \citep{keeney_decisions_1993}, a set of principles for choice well-known in decision theory.

Existing works \citep{carenini_generating_2006, labreuche_general_2011} describe how to generate arguments in natural language that explain, on the basis of an \ac{MAVT} model, why an alternative is preferred to another one. This provides a starting point for the step of generating arguments.
More generally, by picking functions supported by well-studied decision principles, it is made possible to build arguments on the basis of those same principles, and it might be considered that such arguments will have a fighting chance against counter-arguments.

Finally, assume the intuitive behavior of the user is a noisy version of her \ac{DP}. (More elaborated developments would incorporate knowledge from experimental psychology about differences between the usual behavior of users and the one dictated by decision-theoretic models \citep{kahneman_thinking_2013}.)
Then, our task becomes close to one of classical collaborative learning, as \ac{MAVT} defines the class of functions among which to select an optimal one during learning.

The principles on which \ac{MAVT} rests are applicable in specific contexts: those in which the items among which the user aims to choose can be exhaustively described by a set of objective descriptors of the items, known a priori, and which each evaluate the desirability of the items on some aspect. Such aspects are called criteria. Exhaustively described means that the user considers nothing else than the performance of the items on the criteria as relevant for the recommendation.
Thus, consider a set of criteria $\crits$ is known, together with corresponding scales $X_c, c \in \crits$ and descriptors $b_c: \allitems → X_c, c \in \crits$, that each describe the items on a specific criterion $c$.

In such a context, an \ac{MAVT} model representing $\succeq$ is a set of functions $v_c: X_c → \R$ (one for each criterion) such that $\itm \succeq \itm'$ iff $\sum_{c \in C} v_c(b_c(\itm)) ≥ \sum_{c \in C} v_c(b_c(\itm'))$.

Consider as an example the choice of an apartment to rent for holidays. The set of criteria might be: surface of the living room, number of beds, distance to city center, modernity of equipment, presence of a washing machine, price. 
%A minimal condition for these criteria to be appropriate for building an \ac{MAVT} model for a user $\usr$ is that, when two apartments are equally valued on all these criteria (have the same surface of the living room, the same number of beds, and so on), they are equally recommendable. Thus, the set of criteria must completely describe the desirability of an apartment.
Other examples where building an \ac{AR} on the basis of an \ac{MAVT} model may be appropriate include: buying a smartphone, buying a computer, choosing a flight to reach a given place, choosing who to give a “best student” award among a given classroom, locating a new factory, or choosing a university to study at.

By contrast, a recommendation task is not suitable for the approach proposed here if no set of criteria is known that fully describe the desirability of an item. For example, a choice of movie to watch this evening. %Note that a movie is certainly describable on some set of descriptors (for example, the singleton set including its serial number, or a set including its title, release date, and other attributes that together uniquely identify a movie), but those are not criteria as defined above, as those descriptors do not each evaluate the desirability of a movie on some aspect.

%The literature has deeply investigated necessary and sufficient conditions for an \ac{MAVT} model to exist that represent $\succeq$ \citep{k, gonzales}, which will be useful to assess whether it seems reasonable to 

\section{Relation to existing literature and final remarks}
\label{sec:litt}
Approaches exist which propose to estimate a decision-theoretic model that best approximates a user’s observed behavior \citep{greco_trends_2010, sobrie_learning_2018}. 
Furthermore, numerous approaches have been proposed for modeling preferences using machine learning methods \citep{furnkranz_preference_2010}. Both these trends constitute promising areas of investigation for building \acp{AR}.
The idea of using a human as a judge to compare argumentative systems also appears in the paper of \citep{irving_ai_2018}.
There are also articles that are interested in using collaborative filtering on the basis of implicit feedback \citep{rendle_bpr:_2009, hu_collaborative_2008}, an idea somewhat related to ours in the sense that the observed data is not directly the one the model tries to predict.
Also somewhat related are articles interested in collecting preference data for improving the learning \citep{sepliarskaia_preference_2018}.
An important literature analyzes necessary and sufficient conditions for the existence of various decision-theoretic models \citep{krantz_foundations_1971, gonzales_additive_1996, bouyssou_consolidated_2015}.
Finally, this proposal is intimately related to the important literature on explainable AI \citep{DBLP:journals/corr/abs-1804-11192}.

Two final remarks might be useful.
First, I do not mandate that \acp{AR} be able to determine all the items in the \ac{DP} of a user, but merely that the items it claims are in the \ac{DP} of a user be indeed, and that its proofs of this fact be valid, and similarly for the items claimed as not belonging to the \ac{DP} of a user. This is for two reasons: this requirement suffices to yield possibly useful systems (provided $\mitems ≠ \emptyset$), and being able to completely predict the \ac{DP} of a user might reveal extremely difficult. 
As a consequence, \acp{AR} may have very few claims and, correspondingly, face low risks of falsification (as an extreme case, an \ac{AR} that claims nothing is unfalsifiable). This reflects a usual tradeoff in science: general theories are better, but more difficult to get right.

Second, in order to avoid turning recommendation into persuasion or manipulation, 
it is important that the set of all arguments $\allargs$ be not restricted to arguments judged as good arguments from some unique point of view, but rather contains a wide range of arguments promoted by (possibly disagreeing) sources. Accordingly, diverse \acp{AR} should be compared for a given problem in order to span the relevant arguments.

\section*{Acknowledgements} I thank Florian Yger and Jill-Jênn Vie for insightful discussions and the CNRS PEPS program for financial support.

\bibliography{clut,manual}

\appendix
\end{document}
