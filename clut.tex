\RequirePackage[l2tabu, orthodox]{nag}
\documentclass[french, english]{da2pl2018}
\pdfoutput=1
\input{preamble/packages}
\input{preamble/math_basics}
\input{preamble/math_mine}
\input{preamble/redac}
\input{preamble/draw}
\input{preamble/acronyms}

%\setbeamertemplate{headline}[singleline]

\begin{document}
\title{%
	Collaborative learning of models of deliberated preference%
}
\author{Olivier Cailloux}
\affil{Université Paris-Dauphine, PSL Research University, CNRS, LAMSADE, 75016 PARIS, FRANCE\\
	\href{mailto:olivier.cailloux@dauphine.fr}{olivier.cailloux@dauphine.fr}
}
\makeatletter
	\hypersetup{
		pdfsubject={collaborative learning},
		pdfkeywords={machine learning},
		pdfauthor={Olivier Cailloux}
	}
\makeatother
\maketitle

\section{Introduction}
In a situation where a user has to choose an item among a set of possible items, recommender systems aim at recommending some item as most appropriate for the user.
In the collaborative learning community, this is usually taken to mean: recommend an item among the most preferred for the user. In this article, I want to propose another goal for recommender systems. I keep the global aim of recommending some item as most appropriate for a given user, but I propose a different understanding of appropriateness. The goal I propose is to recommend an item among those that are in the deliberated preference of the user, and to prove that it does. The deliberated preference of the user captures what she prefers when considering all arguments in favor or against all possible items. This differs from her intuitive preference in that the latter does not consider arguments. Thus, the goal I propose is appropriate when one is interested to help the user form a deliberated judgement about which item is best, rather than predict which item the user intuitively prefers. Because this will be done using systems that argue, I call the kind of system I define here an \ac{AR}.

Together with this new goal, it is necessary to propose a corresponding measure of the performance of the recommender system (done in \cref{sec:pred}). In the classical approach, measuring the performance of the system amounts to compare its claims (the items it proposes as preferred) to reality (the preference of the user). Because the deliberated preference of the user is not directly observable, this direct check of correspondance becomes non applicable. However, as I will show, the problem of evaluating the quality of an \ac{AR} can be framed as a prediction problem (although involving different objects), thereby recovering the possibility of empirical validation by confrontation between claims of the system and reality.

\Cref{sec:princ} then sketches one possible way of undertaking this task, applicable in specific contexts: recommend on the basis of some explicit principles coming from decision theory. \Cref{sec:litt,sec:disc} provide further references and discussion.

\section{Definition of deliberated preference}
\label{sec:def}
This section describes precisely the new understanding of preference I propose, called the deliberated preference of the user (it is a simplification of the recently proposed concept of deliberated judgement \citep{cailloux_formal_2017}).

I consider given a set of items $\allitems$ among which a user $\usr$ wants to choose. I also consider given a set $\allargs$ containing arguments that may possibly help $\usr$ form a deliberated opinon about which item is best for her (elements of $\allargs$ are not detailed and can be conceived as strings in a natural language). 

I will define the deliberated preference of $\usr$ in that situation as the subset of items that are considered by $\usr$ as having no item strictly preferred to them, considering all possible arguments. To define this properly, I assume we can observe the reaction of $\usr$ to arguments, as follows. 

Define $\ibeats$ as a binary relation over $\allitems × \allargs$, representing the results of the following experiments. Present $(\itm, \ar)$ and $(\itm', \ar')$ to $\usr$ and let her decide, considering the arguments $\ar$ and $\ar'$, which item among $\itm$ and $\itm'$ she prefers, if any. 
Define $(\itm, \ar) \ibeats (\itm', \ar')$ iff $\usr$ strictly prefers $\itm$ to $\itm'$, given $\ar$ and $\ar'$. Note that if no strict preference holds, $\ibeats$ does not relate these two pairs (thus $(\itm, \ar) \nibeats (\itm', \ar')$ and $(\itm', \ar') \nibeats (\itm, \ar)$). 

The deliberated preference $\uitems \subseteq \allitems$ of $\usr$ in that situation contains item $\itm$ iff $\forall (\itm', \ar') \in \allitems × \allargs, \exists \ar \in \allargs \suchthat (\itm', \ar') \nibeats (\itm, \ar)$.

\section{Prediction of deliberated preference}
\label{sec:pred}
An \ac{AR} should be able, given a user, to single out some items as being in his deliberated preference, and some items as being not in his deliberated preference, and to prove its claims by arguing correspondingly. 
Formally, an \ac{AR} $\eta$ has a \emph{scope} $\allusers$, a set of users it claims to be able to predict the deliberated preference of, and gives, for a user $\usr \in \allusers$, a tuple $(\mitems, \mdef, R_\eta, \matt)$, where $\mitems \subseteq \allitems$ is a set of items that $\eta$ claims are among the deliberated preference of $\usr$, $\mdef: \mitems × \allitems → \allargs$ is an argumentation strategy used to defend items in $\mitems$, $R_\eta \subseteq \allitems × \allitems$ is a binary relation that contains pairs of items $(\itm, \itm')$ such that $\eta$ claims that $\usr$ deliberately prefers $\itm$ to $\itm'$, and $\matt: R_\eta → \allargs$ is an argumentation strategy used to support the claims represented by $R_\eta$.

An \ac{AR} is valid if its functions $\mdef$ and $\matt$ indeed justify its claims: when given an item $\itm'$ challenging an item $\itm \in \mitems$ considered by $\eta$ as being in the deliberated preference of $i$, $\mdef$ must produce an argument that successfully defends $\itm$, whatever the argument given in favor of $\itm'$, and similary for $\matt$. More precisely, an \ac{AR} $\eta$ is said to be \emph{correct} about a user $\usr$ iff $\forall \itm \in \mitems, \itm' \in \allitems, \ar' \in \allargs: (\itm', \ar') \nibeats (\itm, \mdef(\itm, \itm'))$ and $\forall (\itm, \itm') \in R_\eta, \ar' \in \allargs: (\itm, \matt(\itm, \itm')) \ibeats (\itm', \ar')$. An \ac{AR} is \emph{valid} iff it is correct about all users in its scope.

Importantly, validity implies the correctness of the claims concerning which items belong or do not belong to the deliberated preference of $\usr$. This fact is stated formally below. (The proof is omitted.)
\begin{fact}
	If an \ac{AR} $\eta$ is valid, then $\forall \usr \in \allusers: \mitems \subseteq \uitems \text{ and } \mnitems \subseteq \allitems \setminus \uitems$, where $\mnitems = R_\eta(\allitems) = \set{\itm' \in \allitems \suchthat (\itm, \itm') \in \R_\eta \text{ for some } \itm \in \allitems}$.
\end{fact}

This notion of validity can be used to \emph{compare} \acp{AR}, in the following way. Given two \acp{AR} $\eta$ and $\eta'$, consider a user $\usr$ in both their scopes and obtain two tuples, $(\mitems, \mdef, R_\eta, \matt)$ and $(\mitems[\eta'], \mdef[\eta'], R_{\eta'}, \matt[\eta'])$. Assume they strongly disagree on the prediction of the deliberated preference of $\usr$, meaning that for some item $\itm \in \allitems$, the first system claims that $\itm$ is in the deliberated preference of $\usr$, thus $\itm \in \mitems$, and the second system claims it is not, thus $\exists \itm' \suchthat (\itm', \itm) \in R_{\eta'}$. Suffices now to let the two systems play against each other and use $\usr$ as a judge. That is, we obtain an argument from the first system in defense of its claim, $\ar = \mdef(\itm, \itm')$, and an argument from the second system, $\ar' = \matt[\eta'](\itm', \itm)$. We present $(\itm, \ar)$ and $(\itm', \ar')$ to $\usr$ and accordingly obtain $(\itm', \ar') \ibeats (\itm, \ar)$ or $(\itm', \ar') \nibeats (\itm, \ar)$. In the first case, the first system is falsified, in the second one, the second system is falsified.

\section{Recommend on the basis of explicit principles}
\label{sec:princ}
Assume we are in a collaborative learning setting, where we have collected observed ratings from users on a set of items, assumed to represent their intuitive rather than deliberated preference. 
A simple approach to build a proof-of-concept \ac{AR} in such a context is to represent the deliberated preference of $\usr$ as a weak order $\succeq \subseteq \allitems × \allitems$, meaning a complete (thus reflexive) and transitive binary relation, whose maximal elements correspond to $\uitems$. (Note however that the definition tolerates other possibilities.)
Assume that the deliberated preference $\succeq$ of the user can be represented by a decision-theoretic model: because such models are conceived for grounding decisions in sound principles, they might be adequate to model deliberated preferences (as opposed to intuitive ones). For concreteness, assume it is \ac{MAVT}, a well-known set of principles for choice coming from decision theory, though other models could be used.

Existing works \citep{carenini_generating_2006, labreuche_general_2011} describes how to generate arguments in natural language that explain, on the basis of an \ac{MAVT} model, why an alternative is preferred to another one. This provides a starting point for the step of generating arguments.
More generally, by picking functions supported by well-studied decision principles, it is made possible to build arguments on the basis of those same principles, and it might be reasonable to think that such arguments will have a fighting chance against counter-arguments.

Finally, assume the intuitive behavior of the user is a noisy version of her deliberated preference. (More elaborated developments of our proposal would incorporate knowledge from experimental psychology about differences between the usual behavior of users and the one dictated by decision-theoretic models.)
Then, our task becomes close to one of classical collaborative learning, as \ac{MAVT} defines the class of functions among which to select an optimal one during learning.

The principles on which \ac{MAVT} rests are applicable in specific contexts, to which we must now focus our attention. In our case of interest, the items among which the user aims to choose can be exhaustively described by a set of objective descriptors of the items, known a priori, and which each evaluate the desirability of the items on some aspect. Such aspects are called criteria. Exhaustively described means that the user considers nothing else than the performance of the items on the criteria as relevant for the recommendation.
Thus, consider a set of criteria $\crits$ is known, together with corresponding scales $X_c, c \in \crits$ and descriptors $b_c: \allitems → X_c, c \in \crits$, that each describe the items on a specific criterion $c$.

In such a context, an \ac{MAVT} model representing $\succeq$ is a set of functions $v_c: X_c → \R$ (one for each criterion) such that $\itm \succeq \itm'$ iff $\sum_{c \in C} v_c(b_c(\itm)) > \sum_{c \in C} v_c(b_c(\itm'))$.

Consider as an example the choice of an apartment to rent for holidays. The set of criteria might be: surface of the living room, number of beds, distance to city center, modernity of equipment, presence of a washing machine, price. 
%A minimal condition for these criteria to be appropriate for building an \ac{MAVT} model for a user $\usr$ is that, when two apartments are equally valued on all these criteria (have the same surface of the living room, the same number of beds, and so on), they are equally recommendable. Thus, the set of criteria must completely describe the desirability of an apartment.
Other examples where building an \ac{AR} on the basis of an \ac{MAVT} model may be appropriate include: buying a smartphone, buying a computer, choosing a flight to reach a given place, or even, possibly, choosing who to give a “best student” award among a given classroom, locating a new factory, or choosing a university to study at.

By contrast, a recommendation task is not suitable for the approach proposed here if no set of criteria is known that fully describe the desirability of an item. For example, a choice of movie to watch this evening. %Note that a movie is certainly describable on some set of descriptors (for example, the singleton set including its serial number, or a set including its title, release date, and other attributes that together uniquely identify a movie), but those are not criteria as defined above, as those descriptors do not each evaluate the desirability of a movie on some aspect.

%The literature has deeply investigated necessary and sufficient conditions for an \ac{MAVT} model to exist that represent $\succeq$ \citep{k, gonzales}, which will be useful to assess whether it seems reasonable to 

\section{Relation to existing literature}
\label{sec:litt}
Approaches exist which propose to estimate a decision-theoretic model that best approximates a user’s observed behavior \citep{greco_trends_2010, sobrie_learning_2018}. 
Furthermore, numerous approaches have been proposed for modeling preferences using machine learning methods \citep{furnkranz_preference_2010}. Both these trends constitute promising areas of investigation for building \acp{AR}.
There are also articles that are interested in using collaborative filtering on the basis of implicit feedback \citep{rendle_bpr:_2009, hu_collaborative_2008}, an idea somewhat related to ours in the sense that the observed data is not directly the one the model tries to predict.
Also somewhat related are articles that are interested in collecting preference data for improving the learning \citep{sepliarskaia_preference_2018}.
An important literature analyzes necessary and sufficient conditions for the existence of various decision-theoretic models \citep{krantz_foundations_1971, gonzales_additive_1996, bouyssou_consolidated_2015}.

\bibliography{clut,manual}

\appendix
\section{Discussion}
\label{sec:disc}
%\subsection{The need for another notion of preference}
In this article we are interested in a notion of preference that possibly changes when confronted to arguments. That is, we assume that the user’s knowledge of which item is best for him is not necessarily fixed a priori, and in particular, that the user may change his mind when presented with arguments in favor of, or counter-arguments against, various items. 

By contrast, the notion of preference considered by classical recommender systems (usually left implicit) in the collaborative learning literature is that the user intuitively knows what he prefers, and that this is the right basis for him to decide. This notion can be related to the conception of preference put forth by \citet{von_neumann_theory_2004} in 1944 in their seminal work. They write that “It is clear that every measurement – or rather every claim of measurability – must ultimately be based on some immediate sensation, which possibly cannot and certainly need not be analyzed any futher. In the case of utility the immediate sensation of preference – of one object or aggregate of objects as against another – provides this basis (…) Let us for the moment accept the picture of an individual whose system of preferences is all-embracing and complete, i.e. who, for any two objects or rather for any two imagined events, possesses a clear intuition of preference. More precisely we expect him, for any two alternative events which are put before him as possibilities, to be able to tell which of the two he prefers.” When applied to collaborative filtering, this view suggests that the job of the system is to help the user single out which items are the best among all the possible ones (to relieve the user from having to search through the whole set). The user, once presented a pair of items (for example, a pair of movies), will know which one he prefers by simple introspection, possibly after having tried them both. Thus, although the user might need help for efficiently searching through a big set of items, he does not need help for the task of comparing a pair of item. Similarly, when the user has already provided some comparisons of preference, this is the final word about those pairs. (Admittedly, it is common to say that a user may make mistakes when reporting her preference, but what this means is usually not specified, and not much is done with this fact, as far as we know, in the literature of collaborative learning.)

We claim that this notion of preference, that we call intuitive preference, is not always the best basis to ground recommendation. Indeed, the user might be unwilling to judge what is best for her on the sole basis of her unaided intuition. First, there are cases where the user can’t easily try out the items. An example is a non repeatable choice, such as choosing a university to study in. Second, in some situations, a notion of fairness is involved, or a similarly abstract notion not easily accessible by non-reflexive perception, such as determining who will receive a prize, or how to best distribute revenue in a society, or to which cause I should donate money. Third, even excluding moral considerations, the best choice might be the result of a complex thought process that should better ensure, as much as possible, that all relevant arguments have been considered, rather than rely on intuition. Consider the decision of which smartphone, or which computer, or which house, to buy. Fourth, the user might appreciate being pointed to some non-salient feature of the items under comparison: for example, a user might feel an intuitive attraction to the plane alternative when chosing a transportation means for holidays, seeing that the flight time is only one hour compared to three hours by train, but change her mind when being reminded that the total travel time should be taken into account. Finally, an important argument coming from psychology can be given in favor of considering arguments rather than intuitive preference to decide on what’s best (in some contexts). It is that unaided intuition is known to be sensible, in some contexts, to framing effects that would perhaps not be considered relevant (by the user himself) when considering arguments and counter-arguments. An example that involves several kinds of the just enumerated concerns is obtaining a procedure to determine to which client a bank should lend money: first, the user probably wants to involve fairness considerations in order to avoid unjust discriminations (which might unconsciously appear if using unaided intuition); second, even if only pure profit considerations are to bear, the user might want the procedure to go beyond reflecting the bare intuition of an expert.
The relation $\ibeats$ is what plays in our approach the role of the basic observable \citeauthor{von_neumann_theory_2004} talk about.

We do not mandate that \acp{AR} be able to determine all the items in the deliberated preference of a user, but merely that the items it claims are in the deliberated preference of a user be indeed, and that its proofs of this fact be valid, and similarly for the items claimed as not belonging to the deliberated preference of a user. This is for two reasons: our requirement is enough to have a possibly useful system (provided $\mitems ≠ \emptyset$), and being able to completely predict the deliberated preference of a user might be extremely difficult. 

\acp{AR} may have very few claims, and accordingly, may have a low chance of being falsified (as an extreme case, an \ac{AR} that claims nothing is unfalsifiable). This reflects the usual difficulty that scientists face: general theories are better, but more difficult to get right.

\end{document}
